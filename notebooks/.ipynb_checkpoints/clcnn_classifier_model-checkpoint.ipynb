{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import MaxPooling1D, Conv1D\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from df_sequence import DfSequence\n",
    "import constants as const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_size = const.ALPHABET_SIZE\n",
    "max_length = const.MAX_LENGTH # Maximum number of characters in an input\n",
    "\n",
    "kernel_width = 3 # Width of filter (Note: there is no height)\n",
    "num_classes = 2 # Number of categories to classify\n",
    "\n",
    "epochs = 4 # Number of times neural network will iterate over entire training set\n",
    "batch_size = 100 # Number of inputs simultaneously fed into the network\n",
    "\n",
    "input_shape = (max_length, alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: even though we are dealing with 1-dimensional data (text) the neural network receives 3-dimensional array of shape \\[maximum_input_length, alphabet_size, batch_size\\].\n",
    "\n",
    "During data preparation step all characters are transliterated (if needed) and encoded as numbers.\n",
    "\n",
    "Let's imagine is that our input is string 'Аба'.\n",
    "\n",
    "So 'Аба' turns into 'aba' and then into \\[1, 2, 1\\] list. The same happens to labels company - 0, person - 1.\n",
    "\n",
    "Just before being fed to neural networks input data is paded to maximum input length and one-hot encoded to avoid creating false relationships, 'a' may be encoded as 1 and 'b' as 2, but in no way 'a' is two times larger than 'b'.\n",
    "\n",
    "So, after one-hot encoding \\[1, 2, 1\\] is turned into (let's imagine that maximum input length is 10 and alphabet size is 5, even though they are actually 150 and 29 )\n",
    "\n",
    "\\[ <br>\n",
    "  \\[1, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 1, 0, 0, 0\\],<br>\n",
    "  \\[1, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 0, 0, 0, 0\\],<br>\n",
    "  \\[0, 0, 0, 0, 0\\]<br>\n",
    "\\]\n",
    "\n",
    "The data is being fed into network in batches. The larger the batch the more memory it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../prepared_data/train_df_f.pkl') # Unpickle training set into Pandas dataframe\n",
    "test_df = pd.read_pickle('../prepared_data/test_df_f.pkl') # Unpickle testing set into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 148, 32)           2816      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 74, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 72, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 36, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              2305000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 2,316,026\n",
      "Trainable params: 2,316,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Here we create our model layer by layer.\n",
    "\n",
    "model = Sequential() # Type of model suitable for less complicated models\n",
    "\n",
    "model.add(Conv1D( # 1-dimensional convolution layer, meaning filter moves only in one dimension\n",
    "                 32, # Number of filters, each filter creates on channel in the following layer\n",
    "                 kernel_size=kernel_width, # Width of the filter, the length is automatically equal to alphabet_size\n",
    "                 strides=1, # Number of characters per step\n",
    "                 activation='relu', # Activation function ignores negative input and does not change positive\n",
    "                 input_shape=input_shape\n",
    "                )\n",
    "         )\n",
    "# Each filter stride outputs only one value,\n",
    "# filter of width 3 can do 148 strides in list with length of 150.\n",
    "# It means that after going through Conv1D layer \n",
    "# the alphabet size dimension is collapsed to 1 and maximum length dimension is 148.\n",
    "# But there are 32 filters.\n",
    "# So, the shape of output is [148, 1, 32] or just [148, 32]\n",
    "# Keras calculates this automatically.\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# MaxPool1D decreases width of input by factor of 2 by selecting largest number from each pair.\n",
    "# Now the shape is [74, 32]\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=kernel_width, activation='relu'))\n",
    "# The same as previous Conv1D layer, but nownumber of filters is 64\n",
    "# and each filter goes over all channels simultaneously.\n",
    "# So the shape is [72, 64]\n",
    "model.add(MaxPooling1D(pool_size=2))# Shape [36, 64]\n",
    "model.add(Flatten()) # Flattens all values into one dimension. Shape 36X64=2304\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "#Dense layer, meaning each neuron from previous layer is connected each meuron in dense leayer.\n",
    "# In this case vua relu activation function.\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#Dense layer with only two neurons.\n",
    "# Softmax function normalizes outputs of all neurons\n",
    "# to be between 0 and 1 and add up to 1.\n",
    "\n",
    "model.summary() # Prints summary of the model. None is the batch size, which is unknown at the monent.\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, # Loss function for classification.\n",
    "              optimizer=keras.optimizers.Adam(), # Magically makes model work better.\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put training and testing dataframes into generators that will perform padding and \n",
    "# one-hot encoding and will supply batches of data of specified shape.\n",
    "train_sequence = DfSequence(train_df, batch_size, max_length, alphabet_size, num_classes)\n",
    "test_sequence = DfSequence(test_df, batch_size, max_length, alphabet_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1138/1138 [==============================] - 77s 68ms/step - loss: 0.0645 - acc: 0.9737 - val_loss: 0.0351 - val_acc: 0.9863\n",
      "Epoch 2/4\n",
      "1138/1138 [==============================] - 74s 65ms/step - loss: 0.0300 - acc: 0.9891 - val_loss: 0.0336 - val_acc: 0.9875\n",
      "Epoch 3/4\n",
      "1138/1138 [==============================] - 76s 66ms/step - loss: 0.0221 - acc: 0.9921 - val_loss: 0.0281 - val_acc: 0.9900\n",
      "Epoch 4/4\n",
      "1138/1138 [==============================] - 82s 72ms/step - loss: 0.0181 - acc: 0.9936 - val_loss: 0.0297 - val_acc: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6ea674dd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training of the model\n",
    "model.fit_generator(train_sequence,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_sequence)\n",
    "# We should experimentally increase number of epochs, utnill\n",
    "# both testing accuracy (val_acc) and training accuracy (acc) \n",
    "# are inreasing together. As soon as testing accuracy starts \n",
    "# decreasing, while training accuracy continue to increase,\n",
    "# we should stop and use model from previous training epoch\n",
    "# to avoid overfiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.03253412965392206\n",
      "Test accuracy: 0.988556068515164\n"
     ]
    }
   ],
   "source": [
    "# Validate model with data never seen before data set.\n",
    "validation_df = pd.read_pickle('../prepared_data/validation_df_f.pkl')\n",
    "validation_sequence = DfSequence(validation_df, batch_size, max_length, alphabet_size, num_classes)\n",
    "\n",
    "score = model.evaluate_generator(validation_sequence, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/company_person_kg.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
